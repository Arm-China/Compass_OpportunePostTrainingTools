[Common]
#the paths for this model's IR
graph = ./squeezenet_s.txt
bin = ./squeezenet_s.bin
model_name = squeezenet_caffe
#the name of dataset plugin for this model's input dataset
#if omitted, will use all zeros as input data for executing forward
dataset = numpynhwcrgb2ncbgrhwdataset
#the path of dataset used for calibration during quantization
#if omitted, will use all zeros as input data for executing calibration
calibration_data = ./calibration2.npy
#the batch_size used for calibration during quantization
calibration_batch_size = 1
#the name of metric plugins for computing accuracy metrics for this model
#if omitted, will not computing accuracy metrics
metric = TopKMetric
#the path of dataset (and corresponding labels) used for computing accuracy metrics for this model
#if ommitted, will not computing accuracy metrics
data = ./validation10.npy
label = ./vlabel10.npy
#the batch_size used for computing accuracy metrics for this model
metric_batch_size = 2
#the quantization method used for weights, default to 'per_tensor_symmetric_restricted_range'
quantize_method_for_weight = per_channel_symmetric_restricted_range
#the quantization method used for activations, default to 'per_tensor_symmetric_full_range'
quantize_method_for_activation = per_tensor_asymmetric
#the bits used for quantizing weight tensors, default to 8
weight_bits = 8
#the bits used for quantizing bias tensors, default to 32
bias_bits = 32
#the bits used for quantizing activation tensors, default to 8
activation_bits = 8
#Maximal LUT items (in bits, as only support LUT with 2**N items) amount when representing nonlinear functions in quantization, 
#default to 8, suggest to set to 10+ when quantizing activations to 16bit
lut_items_in_bits = 8
#the output directory path, default to pwd
output_dir = ./
#the dataloader thread numbers for torch dataset, default to 0, 
#which means do not using multi-threads to accelerate data loading
dataloader_workers=4

